{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Phase 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimporting\n",
    "\n",
    "First, let's reimport all of the data we finalized during our EDA session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./project_data/USVideos.csv\")\n",
    "\n",
    "with open(\"./project_data/US_category_id.json\") as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "cat_map = {}\n",
    "for index,cat in enumerate(categories[\"items\"]):\n",
    "    cat_map[int(cat[\"id\"])]=cat[\"snippet\"][\"title\"]\n",
    "    \n",
    "df[\"category\"] = df[\"category_id\"].map(cat_map)\n",
    "\n",
    "data = df[[\"title\",\"channel_title\",\"category\",\"description\",\"tags\",\"views\"]]\n",
    "\n",
    "data[\"log_views\"] = np.log(data[\"views\"]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take stock of what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split our dataset into features and class variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"views\",axis=1)\n",
    "Y = data[\"log_views\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if they're together\n",
    "X[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your training data to fit any transformers or encoder your need, then apply the fit transformer to your test data. This applies to:\n",
    "* Normalizing/standardizing your features\n",
    "* Using Bag of Words or TF-IDF to encode strings\n",
    "* PCA or dimensionality reduction\n",
    "\n",
    "**Rationale**: In practice, we won't be able to see the test data we'll be making predicting for, so we shouldn't use that data as the basis for any transformation or feature extraction.\n",
    "\n",
    "Here, we **only take the top N** words when doing our count vectors. We do this because to use the full word list would be infeasable since there are so many. Also, due to the nature of language, there are probably only a few words that are used frequently - but many words used only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies our feature transformation to our training and test data\n",
    "# top_n\n",
    "\n",
    "def apply_feature_transformation(X_train, X_test,top_n=100):\n",
    "    # First, create and fit our vectorizers on the training data\n",
    "    # We only take the top_n words into consideration, and simply ignore anything outside of that\n",
    "    tag_vectorizer = CountVectorizer(token_pattern=r\"([^|]+)\", max_features=top_n)\n",
    "    tag_vectors = tag_vectorizer.fit_transform(X_train[\"tags\"].str.replace(\"\\\"\",\"\")).toarray()\n",
    "\n",
    "    description_vectorizer = CountVectorizer(max_features=top_n)\n",
    "    description_vectors = description_vectorizer.fit_transform(X_train[\"description\"].values.astype('U')).toarray()\n",
    "\n",
    "    title_vectorizer = CountVectorizer(max_features=top_n)\n",
    "    title_vectors = title_vectorizer.fit_transform(X_train[\"title\"]).toarray()\n",
    "\n",
    "    # Next, we concatinate the tag, description, and title vectors into one vector\n",
    "    # This is so we can have one \"object\" that we can fit into our models\n",
    "    # Therefore, this will create a matrix of size [num_train_instances * (3 * top_n)]\n",
    "    X_train_transfomed = np.concatenate((tag_vectors, description_vectors, title_vectors), axis=1)\n",
    "    \n",
    "    # Now we fit our test data\n",
    "    # This will create a matrix of size [num_test_instances * (3 * top_n)]\n",
    "    X_test_transformed = np.concatenate(\n",
    "        (\n",
    "            tag_vectorizer.transform(X_test[\"tags\"].str.replace(\"\\\"\",\"\")).toarray(),\n",
    "            description_vectorizer.transform(X_test[\"description\"].values.astype('U')).toarray(),\n",
    "            title_vectorizer.transform(X_test[\"title\"]).toarray()\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return X_train_transfomed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = apply_feature_transformation(X_train_raw,X_test_raw,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to make sure we shaped our features right\n",
    "# There should be 3*top_n features\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Models\n",
    "\n",
    "Now that we have our data fully preprocessed, let's try it with some different models to see what we can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "It's always good to work your way up from the simplest models you know before trying the more complicated ones. So let's see if a simple linear regression will solve our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training score\", reg.score(X_train,y_train))\n",
    "print(\"Testing score:\", reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eugh. Looks like linear regression isn't powerful enough for this problem. There's no use trying to tune this model if even the base accuracy doesn't look promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the KNN Regression takes some time, so we'll only evaluate the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing score:\", neigh.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Regression\n",
    "\n",
    "Let's see if we can squeeze anything out of a neural network. (What you didn't see was fiddling with the params and layer topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(max_iter=45, hidden_layer_sizes = (300,100,50,10))\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To continue, let's choose the KNN model**. This model is semi-interpretable, so it's perfered over the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "For the sake of time, we'll only be finding hyperparams for our KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input: The training X features and Y labels/values\n",
    "    Output: The classifier with the best hyperparams, the predictions\n",
    "    \"\"\"\n",
    "    neigh = KNeighborsRegressor()\n",
    "    param_grid = {\"n_neighbors\": [1,2],\n",
    "                  \"p\": [1,2]}\n",
    "    \n",
    "    # Warning, takes a while!\n",
    "    search = GridSearchCV(neigh, param_grid)\n",
    "    search.fit(X_train,y_train)\n",
    "    return search, search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, predictions = find_best_hyperparameters(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
